{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\duyma\\.cache\\huggingface\\hub\\models--BAAI--bge-m3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 2.00 GiB of which 0 bytes is free. Of the allocated memory 1.72 GiB is allocated by PyTorch, and 4.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\duyma\\OneDrive\\Tài liệu\\GitHub\\Chatbot\\RERE.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duyma/OneDrive/Ta%CC%80i%20li%C3%AA%CC%A3u/GitHub/Chatbot/RERE.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/duyma/OneDrive/Ta%CC%80i%20li%C3%AA%CC%A3u/GitHub/Chatbot/RERE.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# #Load the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/duyma/OneDrive/Ta%CC%80i%20li%C3%AA%CC%A3u/GitHub/Chatbot/RERE.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# base = models.Transformer('qna')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/duyma/OneDrive/Ta%CC%80i%20li%C3%AA%CC%A3u/GitHub/Chatbot/RERE.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# pooling = models.Pooling(1024, pooling_mode='cls')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/duyma/OneDrive/Ta%CC%80i%20li%C3%AA%CC%A3u/GitHub/Chatbot/RERE.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# model = SentenceTransformer(modules=[base, pooling])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/duyma/OneDrive/Ta%CC%80i%20li%C3%AA%CC%A3u/GitHub/Chatbot/RERE.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mBAAI/bge-m3\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/duyma/OneDrive/Ta%CC%80i%20li%C3%AA%CC%A3u/GitHub/Chatbot/RERE.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mBAAI/bge-m3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/duyma/OneDrive/Ta%CC%80i%20li%C3%AA%CC%A3u/GitHub/Chatbot/RERE.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m Embeddings\n",
      "File \u001b[1;32mc:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\transformers\\modeling_utils.py:2014\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2009\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2010\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2011\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2012\u001b[0m     )\n\u001b[0;32m   2013\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2014\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 2.00 GiB of which 0 bytes is free. Of the allocated memory 1.72 GiB is allocated by PyTorch, and 4.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# from sentence_transformers import SentenceTransformer, models\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# #Load the model\n",
    "# base = models.Transformer('qna')\n",
    "# pooling = models.Pooling(1024, pooling_mode='cls')\n",
    "# model = SentenceTransformer(modules=[base, pooling])\n",
    "model = AutoModel.from_pretrained('BAAI/bge-m3').to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-m3')\n",
    "\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_core.documents import BaseDocumentCompressor\n",
    "# from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from langchain_core.pydantic_v1 import Extra, root_validator\n",
    "\n",
    "def top_k(array, k):\n",
    "    indices = np.argsort(array)[::-1][:k]  # Sort the indices in descending order and take top k\n",
    "    return array[indices], indices\n",
    "\n",
    "\n",
    "\n",
    "class AIEmbeddings(Embeddings):\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        encoded = tokenizer(\n",
    "            [text],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        encoded = {k: v.to('cuda') for k, v in encoded.items()}\n",
    "        with torch.no_grad():\n",
    "            embed = model(**encoded, return_dict=True).last_hidden_state[:, 0]\n",
    "            embed = torch.nn.functional.normalize(embed, dim=-1)\n",
    "        return embed[0].tolist()\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        all_embeds = []\n",
    "        docs = list(texts)\n",
    "        for doc in docs:\n",
    "            encoded = tokenizer(\n",
    "                doc,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            print(len(encoded['input_ids'][0]))\n",
    "            encoded = {k: v.to('cuda') for k, v in encoded.items()}\n",
    "            with torch.no_grad():\n",
    "                embed = model(**encoded, return_dict=True).last_hidden_state[:, 0]\n",
    "                embed = torch.nn.functional.normalize(embed, dim=-1)\n",
    "            all_embeds.append(embed[0].tolist())\n",
    "        return all_embeds\n",
    "    \n",
    "import pandas as pd\n",
    "from langchain.schema import Document #Format documents\n",
    "\n",
    "\n",
    "sheet_list = [\"Tài liệu - Quy định vận chuyển\", \"Tài liệu -  Quy định chung\", \"Tài liệu - Chính sách giá\", \"Tài liệu - Chính sách bồi hoàn\", \"Tài liệu - Ví\"]\n",
    "documents = list()\n",
    "for sheet in sheet_list:\n",
    "  df = pd.read_excel(\"data/Q&A.xlsx\", sheet_name=sheet, skiprows=[0])\n",
    "  text_1 = sheet\n",
    "  text_2 = list(df['Mục'])\n",
    "  text_3 = list(df['Nội dung'])\n",
    "  titles = [f'{text_1}\\n{text_2[i]}' for i in range(len(df))]\n",
    "  contents = [f'{text_1}\\n{text_2[i]}\\n{text_3[i]}' for i in range(len(df))]\n",
    "  for title, content in zip(titles, contents):\n",
    "    #   print(content)\n",
    "    #   print('-'*100)\n",
    "      document = Document(\n",
    "          page_content=content.strip(),\n",
    "          metadata={\n",
    "              'title': title\n",
    "          }\n",
    "      )\n",
    "      documents.append(document)\n",
    "\n",
    "\n",
    "class core_chat():\n",
    "    def __init__(self, documents, embeddings):\n",
    "        self.docs = documents\n",
    "        self.embeddings = embeddings\n",
    "        self.k = 3\n",
    "        \n",
    "\n",
    "    def setup_retriever(self):\n",
    "\n",
    "\n",
    "        vectorstore = FAISS.from_documents(self.docs, self.embeddings)\n",
    "        faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": self.k})\n",
    "\n",
    "        self.ensemble_retriever = faiss_retriever\n",
    "        return self.ensemble_retriever\n",
    "\n",
    "    def get_relevant(self, question):\n",
    "        docs_query = self.ensemble_retriever.get_relevant_documents(question)\n",
    "\n",
    "        return docs_query\n",
    "    \n",
    "embeddings = AIEmbeddings()\n",
    "\n",
    "core = core_chat(documents, embeddings)\n",
    "retriever = core.setup_retriever()\n",
    "\n",
    "onlyquestion = pd.read_csv(\"data/Clean_data_small.csv\")\n",
    "\n",
    "count = 0\n",
    "onlyquestion[\"check\"] = 0\n",
    "for i in tqdm(onlyquestion.index):\n",
    "    retrieved_docs = core.get_relevant(onlyquestion[\"question\"][i])\n",
    "    ret_title = [doc.metadata[\"title\"] for doc in retrieved_docs]\n",
    "    if onlyquestion[\"title\"][i] in ret_title[0:3]:\n",
    "        onlyquestion.loc[i,\"check\"] = 1\n",
    "\n",
    "print(onlyquestion[\"check\"].value_counts())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
