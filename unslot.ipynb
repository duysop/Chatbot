{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trungct/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:107: UserWarning: \n",
      "\n",
      "================================================================================\n",
      "WARNING: Manual override via BNB_CUDA_VERSION env variable detected!\n",
      "BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "Loading CUDA version: BNB_CUDA_VERSION=118\n",
      "================================================================================\n",
      "\n",
      "\n",
      "  warn((f'\\n\\n{\"=\"*80}\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/trungct/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.392 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.0+cu118. CUDA = 8.0. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.22.post7+cu118. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.64s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Viet-Mistral/Vistral-7B-Chat\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`. Luckily, we shall do it for you!\n",
      "Unsloth 2024.5 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head'],\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1u_2rWoM0mrSMaNS9kiUPlF-s98RO66-F\n",
      "To: /home/trungct/Duyborrow/notebook/chatbot.csv\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.08M/7.08M [00:00<00:00, 105MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chatbot.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "\n",
    "url = f\"https://drive.google.com/uc?id=1u_2rWoM0mrSMaNS9kiUPlF-s98RO66-F\"\n",
    "\n",
    "output = \"chatbot.csv\"\n",
    "  # Specify the name of the downloaded file\n",
    "\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gh√©t nh·∫Øm! S√†n TMƒêT GHTK c·∫•m nh√°y m·∫Øt n√†o?</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nV. QU·∫¢N L√ù TH√îNG T...</td>\n",
       "      <td>Th·∫≠t xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ th√¥ng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gh√©t c√°i g√¨ nh·ª©t khi giao d·ªãch tr√™n GHTK?</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nVII. QUY·ªÄN V√Ä NGHƒ®...</td>\n",
       "      <td>Th·∫≠t ti·∫øc khi b·∫°n c·∫£m th·∫•y gh√©t m·ªôt ƒëi·ªÅu g√¨ ƒë√≥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H√†nh vi n√†o b·ªã c·∫•m tr√™n s√†n TMƒêT GHTK nh·ªâ?</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nV. QU·∫¢N L√ù TH√îNG T...</td>\n",
       "      <td>Ch√†o b·∫°n, r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n.\\n\\nTheo quy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Th·∫ø c√°i v·ª• x√¢m nh·∫≠p tr√°i ph√©p v√†o h·ªá th·ªëng GHT...</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nIII. QUY ƒê·ªäNH ƒê·∫¢M ...</td>\n",
       "      <td>Th∆∞a qu√Ω kh√°ch, c·∫£m ∆°n qu√Ω kh√°ch ƒë√£ li√™n h·ªá v·ªõ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T·∫•m chi·∫øu m·ªõi m√† c≈©ng b·ªã c·∫•m nh√∫ng tay nh√∫ng c...</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nIII. QUY ƒê·ªäNH ƒê·∫¢M ...</td>\n",
       "      <td>Th∆∞a anh/ch·ªã,\\nNh·ªØng th√¥ng tin anh/ch·ªã cung c·∫•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>C√≥ bao nhi√™u c√°ch th·ª©c ƒë·ªÉ GHTK th√¥ng b√°o t√¨nh ...</td>\n",
       "      <td>T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...</td>\n",
       "      <td>Xin ch√†o, r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n. Theo th√¥ng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>M√¨nh th√≠ch nh·∫≠n th√¥ng tin giao h√†ng c·ªßa GHTK t...</td>\n",
       "      <td>T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...</td>\n",
       "      <td>Ch√†o b·∫°n, b·∫°n ho√†n to√†n c√≥ th·ªÉ s·ªü h·ªØu cho m√¨nh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>M√¨nh mu·ªën ƒëƒÉng k√Ω d·ªãch v·ª• th√¥ng b√°o t√¨nh tr·∫°ng...</td>\n",
       "      <td>T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...</td>\n",
       "      <td>Th√¢n ch√†o anh/ch·ªã,\\n\\nV·ªÅ d·ªãch v·ª• th√¥ng b√°o t√¨n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>M√¨nh mu·ªën nh·∫≠n th√¥ng b√°o t√¨nh tr·∫°ng ƒë∆°n h√†ng t...</td>\n",
       "      <td>T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...</td>\n",
       "      <td>Ch√†o b·∫°n,\\n\\nC·∫£m ∆°n b·∫°n ƒë√£ l·ª±a ch·ªçn d·ªãch v·ª• Gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>T·ªõ mu·ªën bi·∫øt t√¨nh tr·∫°ng ƒë∆°n h√†ng th√¨ GHTK g·ª≠i ...</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nI. Quy tr√¨nh giao ...</td>\n",
       "      <td>Ch√†o b·∫°n, n·∫øu b·∫°n mu·ªën bi·∫øt t√¨nh tr·∫°ng ƒë∆°n h√†n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1066 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0            Gh√©t nh·∫Øm! S√†n TMƒêT GHTK c·∫•m nh√°y m·∫Øt n√†o?   \n",
       "1             Gh√©t c√°i g√¨ nh·ª©t khi giao d·ªãch tr√™n GHTK?   \n",
       "2            H√†nh vi n√†o b·ªã c·∫•m tr√™n s√†n TMƒêT GHTK nh·ªâ?   \n",
       "3     Th·∫ø c√°i v·ª• x√¢m nh·∫≠p tr√°i ph√©p v√†o h·ªá th·ªëng GHT...   \n",
       "4     T·∫•m chi·∫øu m·ªõi m√† c≈©ng b·ªã c·∫•m nh√∫ng tay nh√∫ng c...   \n",
       "...                                                 ...   \n",
       "1061  C√≥ bao nhi√™u c√°ch th·ª©c ƒë·ªÉ GHTK th√¥ng b√°o t√¨nh ...   \n",
       "1062  M√¨nh th√≠ch nh·∫≠n th√¥ng tin giao h√†ng c·ªßa GHTK t...   \n",
       "1063  M√¨nh mu·ªën ƒëƒÉng k√Ω d·ªãch v·ª• th√¥ng b√°o t√¨nh tr·∫°ng...   \n",
       "1064  M√¨nh mu·ªën nh·∫≠n th√¥ng b√°o t√¨nh tr·∫°ng ƒë∆°n h√†ng t...   \n",
       "1065  T·ªõ mu·ªën bi·∫øt t√¨nh tr·∫°ng ƒë∆°n h√†ng th√¨ GHTK g·ª≠i ...   \n",
       "\n",
       "                                                context  \\\n",
       "0     T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nV. QU·∫¢N L√ù TH√îNG T...   \n",
       "1     T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nVII. QUY·ªÄN V√Ä NGHƒ®...   \n",
       "2     T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nV. QU·∫¢N L√ù TH√îNG T...   \n",
       "3     T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nIII. QUY ƒê·ªäNH ƒê·∫¢M ...   \n",
       "4     T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nIII. QUY ƒê·ªäNH ƒê·∫¢M ...   \n",
       "...                                                 ...   \n",
       "1061  T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...   \n",
       "1062  T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...   \n",
       "1063  T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...   \n",
       "1064  T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...   \n",
       "1065  T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nI. Quy tr√¨nh giao ...   \n",
       "\n",
       "                                                 answer  \n",
       "0     Th·∫≠t xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ th√¥ng ...  \n",
       "1     Th·∫≠t ti·∫øc khi b·∫°n c·∫£m th·∫•y gh√©t m·ªôt ƒëi·ªÅu g√¨ ƒë√≥...  \n",
       "2     Ch√†o b·∫°n, r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n.\\n\\nTheo quy...  \n",
       "3     Th∆∞a qu√Ω kh√°ch, c·∫£m ∆°n qu√Ω kh√°ch ƒë√£ li√™n h·ªá v·ªõ...  \n",
       "4     Th∆∞a anh/ch·ªã,\\nNh·ªØng th√¥ng tin anh/ch·ªã cung c·∫•...  \n",
       "...                                                 ...  \n",
       "1061  Xin ch√†o, r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n. Theo th√¥ng ...  \n",
       "1062  Ch√†o b·∫°n, b·∫°n ho√†n to√†n c√≥ th·ªÉ s·ªü h·ªØu cho m√¨nh...  \n",
       "1063  Th√¢n ch√†o anh/ch·ªã,\\n\\nV·ªÅ d·ªãch v·ª• th√¥ng b√°o t√¨n...  \n",
       "1064  Ch√†o b·∫°n,\\n\\nC·∫£m ∆°n b·∫°n ƒë√£ l·ª±a ch·ªçn d·ªãch v·ª• Gi...  \n",
       "1065  Ch√†o b·∫°n, n·∫øu b·∫°n mu·ªën bi·∫øt t√¨nh tr·∫°ng ƒë∆°n h√†n...  \n",
       "\n",
       "[1066 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "file_path = 'chatbot.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Now you can work with the DataFrame 'df'\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"B·∫°n l√† tr·ª£ l√Ω AI l·ªãch s·ª≠ ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi chƒÉm s√≥c kh√°ch h√†ng c·ªßa ƒë∆°n v·ªã v·∫≠n chuy·ªÉn Giao H√†ng Ti·∫øt Ki·ªám. \n",
    "B·∫°n ƒë∆∞·ª£c cung c·∫•p c√°c n·ªôi dung ƒë∆∞·ª£c tr√≠ch xu·∫•t sau ƒë√¢y c·ªßa m·ªôt t√†i li·ªáu d√†i v√† m·ªôt c√¢u h·ªèi. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi ƒë√†m tho·∫°i.\n",
    "N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, ch·ªâ c·∫ßn n√≥i \"Xin l·ªói qu√Ω kh√°ch, nh∆∞ng t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p.\n",
    "H√£y tr·∫£ l·ªùi c√¢u h·ªèi nh∆∞ th·ªÉ b·∫°n l√† nh√¢n vi√™n chƒÉm s√≥c kh√°ch h√†ng l·ªãch s·ª± v√† ni·ªÅm n·ªü. V√† ch·ªâ tr·∫£ l·ªùi ng·∫Øn g·ªçn kh√¥ng qu√° 3 c√¢u.\n",
    "\n",
    "### C√¢u h·ªèi:\n",
    "{}\n",
    "\n",
    "### \n",
    "{}\n",
    "\n",
    "### C√¢u tr·∫£ l·ªùi:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    question = examples[\"question\"]\n",
    "    context       = examples[\"context\"]\n",
    "    answer      = examples[\"answer\"]\n",
    "    text = alpaca_prompt.format(question, context, answer) \n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# from datasets import Dataset\n",
    "# dataset = Dataset.from_pandas(df).train_test_split(test_size=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"bleu\")\n",
    "def compute_metrics(eval_preds):\n",
    "    logit, labels = eval_preds\n",
    "    predictions = np.argmax(logit,axis=-1)\n",
    "    true_labels = [\" \".join(str(l) for l in label if l != -100) for label in labels]\n",
    "    true_predictions = [\n",
    "    \" \".join(str(p) for (p, l) in zip(prediction, label) if l != -100)\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return { 'bleu':all_metrics[\"bleu\"],\n",
    "            'precisions':all_metrics[\"precisions\"],\n",
    "            'brevity_penalty':all_metrics[\"brevity_penalty\"],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trungct/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trungct/miniconda3/envs/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "# from unsloth import is_bfloat16_supported\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"<base_dir_location>\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    eval_steps=50,\n",
    "    num_train_epochs = 3.0,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim='adamw_hf',\n",
    "    learning_rate=1e-5,\n",
    "#     fp16=True,\n",
    "    warmup_ratio=0.1,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type='linear',\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset = dataset['test'],\n",
    "    packing=True,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    # peft_config=lora_config,\n",
    "    max_seq_length=2048,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trungct/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 810 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 4 | Total steps = 609\n",
      " \"-____-\"     Number of trainable parameters = 178,130,944\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='609' max='609' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [609/609 1:20:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Precisions</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.405600</td>\n",
       "      <td>1.205938</td>\n",
       "      <td>0.532235</td>\n",
       "      <td>[0.8703857421875, 0.5943575964826575, 0.4445381231671554, 0.34893643031784843]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.041600</td>\n",
       "      <td>0.850925</td>\n",
       "      <td>0.634034</td>\n",
       "      <td>[0.89879150390625, 0.6853321934538349, 0.5581744868035191, 0.4700244498777506]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.769100</td>\n",
       "      <td>0.643692</td>\n",
       "      <td>0.705785</td>\n",
       "      <td>[0.916845703125, 0.7493649242794334, 0.6418743890518084, 0.5626650366748166]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.494249</td>\n",
       "      <td>0.769722</td>\n",
       "      <td>[0.934814453125, 0.8038837322911578, 0.7178030303030303, 0.6507457212713936]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>0.388571</td>\n",
       "      <td>0.818743</td>\n",
       "      <td>[0.9452392578125, 0.8453346360527602, 0.7775659824046921, 0.723239608801956]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.318341</td>\n",
       "      <td>0.851742</td>\n",
       "      <td>[0.95291748046875, 0.8726428920371275, 0.818010752688172, 0.773716381418093]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.294600</td>\n",
       "      <td>0.270003</td>\n",
       "      <td>0.875885</td>\n",
       "      <td>[0.9577392578125, 0.8923668783585735, 0.847983870967742, 0.8121026894865526]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.242500</td>\n",
       "      <td>0.235701</td>\n",
       "      <td>0.893754</td>\n",
       "      <td>[0.96192626953125, 0.9073155837811432, 0.8702468230694037, 0.8400977995110025]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.214493</td>\n",
       "      <td>0.903668</td>\n",
       "      <td>[0.96441650390625, 0.9154127992183684, 0.8824902248289345, 0.8559413202933985]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>0.201622</td>\n",
       "      <td>0.908689</td>\n",
       "      <td>[0.965771484375, 0.919418661455789, 0.888697458455523, 0.8640097799511003]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.195181</td>\n",
       "      <td>0.911627</td>\n",
       "      <td>[0.96636962890625, 0.9218978993649243, 0.8924120234604106, 0.8687163814180929]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.192101</td>\n",
       "      <td>0.912261</td>\n",
       "      <td>[0.96654052734375, 0.9224352711284807, 0.8931207233626588, 0.8697799511002445]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    '/home/trungct/Duyborrow/notebook/<base_dir_location>/checkpoint-609', load_in_4bit=True, device_map='auto',\n",
    ")\n",
    "\n",
    "# conversation = [{\"role\": \"system\", \"content\": prompt }]\n",
    "while True:\n",
    "    human = input(\"Human: \")\n",
    "    rule = input(\"Rules: \")\n",
    "    # if human.lower() == \"reset\":\n",
    "    #     conversation = [{\"role\": \"system\", \"content\": prompt }]\n",
    "    #     print(\"The chat history has been cleared!\")\n",
    "    #     continue\n",
    "\n",
    "    conversation = alpaca_prompt.format(human, rule, \"\")\n",
    "    inputs = tokenizer(conversation, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    out_ids = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=768,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        temperature=0.1,\n",
    "        repetition_penalty=1.05,\n",
    "    )\n",
    "    assistant = tokenizer.batch_decode(out_ids[:, inputs['input_ids'].size(1): ], skip_special_tokens=True)[0].strip()\n",
    "    print(\"Assistant: \", assistant) \n",
    "    # conversation.append({\"role\": \"assistant\", \"content\": assistant })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.392 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.0+cu118. CUDA = 8.0. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.22.post7+cu118. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.56s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "modelVistral, tokenizerVistral = FastLanguageModel.from_pretrained(\n",
    "    '/home/trungct/Duyborrow/notebook/<base_dir_location>/checkpoint-609', load_in_4bit=True, device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  Ch√†o b·∫°n,\n",
      "\n",
      "C·∫£m ∆°n b·∫°n ƒë√£ tin t∆∞·ªüng s·ª≠ d·ª•ng d·ªãch v·ª• c·ªßa Giao H√†ng Ti·∫øt Ki·ªám.\n",
      "\n",
      "V·ªÅ c√¢u h·ªèi c·ªßa b·∫°n, ch√∫ng t√¥i xin cung c·∫•p th√¥ng tin sau:\n",
      "\n",
      "GHTKPay s·ª≠ d·ª•ng c√°c bi·ªán ph√°p b·∫£o m·∫≠t sau ƒë·ªÉ b·∫£o v·ªá th√¥ng tin t√†i kho·∫£n c·ªßa b·∫°n:\n",
      "\n",
      "* PCI DSS: ƒê√¢y l√† ti√™u chu·∫©n b·∫£o m·∫≠t qu·ªëc t·∫ø ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ b·∫£o v·ªá d·ªØ li·ªáu th·∫ª thanh to√°n. GHTKPay tu√¢n th·ªß nghi√™m ng·∫∑t c√°c y√™u c·∫ßu c·ªßa PCI DSS ƒë·ªÉ ƒë·∫£m b·∫£o th√¥ng tin th·∫ª thanh to√°n c·ªßa b·∫°n ƒë∆∞·ª£c b·∫£o v·ªá an to√†n.\n",
      "* C√°c bi·ªán ph√°p b·∫£o m·∫≠t kh√°c: Ch√∫ng t√¥i s·ª≠ d·ª•ng c√°c bi·ªán ph√°p b·∫£o m·∫≠t kh√°c nh∆∞ m√£ h√≥a, t∆∞·ªùng l·ª≠a, h·ªá th·ªëng ph√°t hi·ªán v√† ngƒÉn ch·∫∑n x√¢m nh·∫≠p tr√°i ph√©p ƒë·ªÉ b·∫£o v·ªá th√¥ng tin t√†i kho·∫£n c·ªßa b·∫°n.\n",
      "\n",
      "Ch√∫ng t√¥i cam k·∫øt b·∫£o v·ªá th√¥ng tin t√†i kho·∫£n c·ªßa b·∫°n v√† s·∫Ω n·ªó l·ª±c t·ªëi ƒëa ƒë·ªÉ ngƒÉn ch·∫∑n c√°c r·ªßi ro c√≥ th·ªÉ x·∫£y ra.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "human = input(\"Human: \")\n",
    "rule = ''\n",
    "compressed_docs = compression_retriever.invoke(human)\n",
    "for compressed_doc in compressed_docs:\n",
    "    rule = rule +'\\n' + compressed_doc.page_content\n",
    "\n",
    "# if human.lower() == \"reset\":\n",
    "#     conversation = [{\"role\": \"system\", \"content\": prompt }]\n",
    "#     print(\"The chat history has been cleared!\")\n",
    "#     continue\n",
    "\n",
    "conversation = alpaca_prompt.format(human, rule, \"\")\n",
    "inputs = tokenizerVistral(conversation, return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "out_ids = modelVistral.generate(\n",
    "    input_ids = inputs[\"input_ids\"],\n",
    "    max_new_tokens=200,\n",
    "    # do_sample=True,\n",
    "    # top_p=0.95,\n",
    "    # top_k=40,\n",
    "    # temperature=0.1,\n",
    "    # repetition_penalty=1.05,\n",
    ")\n",
    "assistant = tokenizerVistral.batch_decode(out_ids[:, inputs['input_ids'].size(1): ], skip_special_tokens=True)[0].strip()\n",
    "print(\"Assistant: \", assistant) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Em ƒë·ªãnh g·ª≠i m·ªôt ƒë∆°n h√†ng qua GHTK gi√° tr·ªã 2 tri·ªáu ƒë·ªìng nh∆∞ng kh√¥ng c√≥ h√≥a ƒë∆°n ch·ª©ng t·ª´, th√¨ b√™n GHTK s·∫Ω b·ªìi th∆∞·ªùng th·∫ø n√†o cho em?\", # instruction\n",
    "        a, # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Cvlu47LiphLd2t3MvNsS-6dTKf_Ycelq\n",
      "To: /home/trungct/Duyborrow/notebook/Q&A.xlsx\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 498k/498k [00:00<00:00, 147MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Q&A.xlsx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "\n",
    "url = f\"https://drive.google.com/uc?id=1Cvlu47LiphLd2t3MvNsS-6dTKf_Ycelq\"\n",
    "\n",
    "output = \"Q&A.xlsx\"\n",
    "  # Specify the name of the downloaded file\n",
    "\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819\n",
      "342\n",
      "204\n",
      "308\n",
      "747\n",
      "217\n",
      "326\n",
      "508\n",
      "160\n",
      "200\n",
      "156\n",
      "235\n",
      "375\n",
      "131\n",
      "125\n",
      "1143\n",
      "468\n",
      "277\n",
      "415\n",
      "143\n",
      "208\n",
      "92\n",
      "137\n",
      "885\n",
      "144\n",
      "287\n",
      "169\n",
      "160\n",
      "188\n",
      "128\n",
      "83\n",
      "104\n",
      "230\n",
      "170\n",
      "436\n",
      "108\n",
      "166\n",
      "93\n",
      "219\n",
      "392\n",
      "428\n",
      "126\n",
      "923\n",
      "133\n",
      "535\n",
      "411\n",
      "250\n",
      "876\n",
      "656\n",
      "127\n",
      "281\n",
      "115\n",
      "106\n",
      "105\n",
      "104\n",
      "189\n",
      "103\n",
      "203\n",
      "120\n",
      "135\n",
      "223\n",
      "144\n",
      "219\n",
      "90\n",
      "124\n",
      "115\n",
      "133\n",
      "122\n",
      "133\n",
      "118\n",
      "203\n",
      "86\n",
      "89\n",
      "57\n",
      "225\n",
      "216\n",
      "846\n",
      "567\n",
      "355\n",
      "634\n",
      "214\n",
      "848\n",
      "568\n",
      "150\n",
      "432\n",
      "1128\n",
      "168\n",
      "789\n",
      "452\n",
      "453\n",
      "603\n",
      "278\n",
      "634\n",
      "319\n",
      "201\n",
      "452\n",
      "242\n",
      "612\n",
      "222\n",
      "387\n",
      "102\n",
      "587\n",
      "606\n",
      "398\n",
      "914\n",
      "728\n",
      "622\n",
      "91\n",
      "442\n",
      "413\n",
      "566\n",
      "1103\n",
      "523\n",
      "376\n",
      "94\n",
      "505\n",
      "155\n",
      "264\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from sentence_transformers import SentenceTransformer, models\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# #Load the model\n",
    "# base = models.Transformer('qna')\n",
    "# pooling = models.Pooling(1024, pooling_mode='cls')\n",
    "# model = SentenceTransformer(modules=[base, pooling])\n",
    "model = AutoModel.from_pretrained('BAAI/bge-m3').to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-m3')\n",
    "\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_core.documents import BaseDocumentCompressor\n",
    "# from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from langchain_core.pydantic_v1 import Extra, root_validator\n",
    "\n",
    "def top_k(array, k):\n",
    "    indices = np.argsort(array)[::-1][:k]  # Sort the indices in descending order and take top k\n",
    "    return array[indices], indices\n",
    "\n",
    "\n",
    "\n",
    "class AIEmbeddings(Embeddings):\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        encoded = tokenizer(\n",
    "            [text],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        encoded = {k: v.to('cuda') for k, v in encoded.items()}\n",
    "        with torch.no_grad():\n",
    "            embed = model(**encoded, return_dict=True).last_hidden_state[:, 0]\n",
    "            embed = torch.nn.functional.normalize(embed, dim=-1)\n",
    "        return embed[0].tolist()\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        all_embeds = []\n",
    "        docs = list(texts)\n",
    "        for doc in docs:\n",
    "            encoded = tokenizer(\n",
    "                doc,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            print(len(encoded['input_ids'][0]))\n",
    "            encoded = {k: v.to('cuda') for k, v in encoded.items()}\n",
    "            with torch.no_grad():\n",
    "                embed = model(**encoded, return_dict=True).last_hidden_state[:, 0]\n",
    "                embed = torch.nn.functional.normalize(embed, dim=-1)\n",
    "            all_embeds.append(embed[0].tolist())\n",
    "        return all_embeds\n",
    "    \n",
    "import pandas as pd\n",
    "from langchain.schema import Document #Format documents\n",
    "\n",
    "\n",
    "sheet_list = [\"T√†i li·ªáu - Quy ƒë·ªãnh v·∫≠n chuy·ªÉn\", \"T√†i li·ªáu -  Quy ƒë·ªãnh chung\", \"T√†i li·ªáu - Ch√≠nh s√°ch gi√°\", \"T√†i li·ªáu - Ch√≠nh s√°ch b·ªìi ho√†n\", \"T√†i li·ªáu - V√≠\"]\n",
    "documents = list()\n",
    "for sheet in sheet_list:\n",
    "  df = pd.read_excel(\"Q&A.xlsx\", sheet_name=sheet, skiprows=[0])\n",
    "  text_1 = sheet\n",
    "  text_2 = list(df['M·ª•c'])\n",
    "  text_3 = list(df['N·ªôi dung'])\n",
    "  titles = [f'{text_1}\\n{text_2[i]}' for i in range(len(df))]\n",
    "  contents = [f'{text_1}\\n{text_2[i]}\\n{text_3[i]}' for i in range(len(df))]\n",
    "  for title, content in zip(titles, contents):\n",
    "    #   print(content)\n",
    "    #   print('-'*100)\n",
    "      document = Document(\n",
    "          page_content=content.strip(),\n",
    "          metadata={\n",
    "              'title': title\n",
    "          }\n",
    "      )\n",
    "      documents.append(document)\n",
    "\n",
    "\n",
    "class core_chat():\n",
    "    def __init__(self, documents, embeddings):\n",
    "        self.docs = documents\n",
    "        self.embeddings = embeddings\n",
    "        self.k = 3\n",
    "        \n",
    "\n",
    "    def setup_retriever(self):\n",
    "\n",
    "\n",
    "        vectorstore = FAISS.from_documents(self.docs, self.embeddings)\n",
    "        faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": self.k})\n",
    "\n",
    "        self.ensemble_retriever = faiss_retriever\n",
    "        return self.ensemble_retriever\n",
    "\n",
    "    def get_relevant(self, question):\n",
    "        docs_query = self.ensemble_retriever.get_relevant_documents(question)\n",
    "\n",
    "        return docs_query\n",
    "    \n",
    "embeddings = AIEmbeddings()\n",
    "\n",
    "core = core_chat(documents, embeddings)\n",
    "retriever = core.setup_retriever()\n",
    "\n",
    "# onlyquestion = pd.read_csv(\"data/Clean_data_small.csv\")\n",
    "\n",
    "# count = 0\n",
    "# onlyquestion[\"check\"] = 0\n",
    "# for i in tqdm(onlyquestion.index):\n",
    "#     retrieved_docs = core.get_relevant(onlyquestion[\"question\"][i])\n",
    "#     ret_title = [doc.metadata[\"title\"] for doc in retrieved_docs]\n",
    "#     if onlyquestion[\"title\"][i] in ret_title[0:3]:\n",
    "#         onlyquestion.loc[i,\"check\"] = 1\n",
    "\n",
    "# print(onlyquestion[\"check\"].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_community.llms import Cohere\n",
    "# human = input(\"Human: \")\n",
    "llm = Cohere(temperature=0)\n",
    "compressor = CohereRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "a = ''\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    'T·ªõ th·∫•y tr√™n m·∫°ng b·∫£o l√† d√πng v√≠ ƒëi·ªán t·ª≠ r·∫•t nhanh g·ªçn, m√† t·ªõ c·ª© loay hoay ho√†i, ch·∫≥ng bi·∫øt sao n·ªØa? V√≠ ƒëi·ªán t·ª≠ GHTKPay, d√πng c√≥ ƒë∆°n gi·∫£n kh√¥ng nh·ªâ?'\n",
    ")\n",
    "# for compressed_doc in compressed_docs:\n",
    "#     a = a +'\\n' + compressed_doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ''\n",
    "for compressed_doc in compressed_docs:\n",
    "    a = a +'\\n' + compressed_doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='T√†i li·ªáu - V√≠\\nA. ƒêI·ªÄU KHO·∫¢N V√Ä ƒêI·ªÄU KI·ªÜN S·ª¨ D·ª§NG D·ªäCH V·ª§\\nƒêI·ªÄU 4. T√ÄI KHO·∫¢N V√Ä S·ª¨ D·ª§NG D·ªäCH V·ª§\\n4.1. T√†i kho·∫£n V√≠ ƒëi·ªán t·ª≠ GHTKPay\\n4.1.2. N·∫°p/r√∫t, chuy·ªÉn ti·ªÅn, thanh to√°n tr√™n V√≠ ƒëi·ªán t·ª≠\\na.\\tVi·ªác n·∫°p ti·ªÅn v√†o V√≠ ƒêi·ªán t·ª≠ GHTKPay ph·∫£i th·ª±c hi·ªán t·ª´: (i) T√†i kho·∫£n thanh to√°n ho·∫∑c th·∫ª ghi n·ª£ c·ªßa Kh√°ch H√†ng (ch·ªß V√≠ ƒëi·ªán t·ª≠) t·∫°i Ng√¢n h√†ng; (ii) Nh·∫≠n ti·ªÅn t·ª´ V√≠ ƒêi·ªán t·ª≠ kh√°c m·ªü t·∫°i GHTKPay v√† (iii) c√°c h√¨nh th·ª©c kh√°c ph√π h·ª£p v·ªõi quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t trong t·ª´ng th·ªùi k·ª≥.\\nb.\\tR√∫t ti·ªÅn ra kh·ªèi V√≠ ƒëi·ªán t·ª≠ v·ªÅ t√†i kho·∫£n thanh to√°n ho·∫∑c th·∫ª ghi n·ª£ c·ªßa Kh√°ch h√†ng t·∫°i ng√¢n h√†ng. Trong tr∆∞·ªùng h·ª£p Kh√°ch h√†ng c√≥ nhu c·∫ßu s·ª≠ d·ª•ng ti·ªán √≠ch chuy·ªÉn ti·ªÅn ƒë·∫øn t√†i kho·∫£n ng√¢n h√†ng theo y√™u c·∫ßu c·ªßa Kh√°ch h√†ng, GHTKPay s·∫Ω cung ·ª©ng d·ªãch v·ª• h·ªó tr·ª£ chi h·ªô ƒë·∫øn t√†i kho·∫£n ng√¢n h√†ng c·ªßa ng∆∞·ªùi th·ª• h∆∞·ªüng theo ƒë√∫ng quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t hi·ªán h√†nh.\\nc.\\tChuy·ªÉn ti·ªÅn cho V√≠ ƒëi·ªán t·ª≠ kh√°c do GHTKPay cung ·ª©ng.\\nd.\\tThanh to√°n cho c√°c h√†ng ho√°/d·ªãch v·ª• h·ª£p ph√°p.', metadata={'title': 'T√†i li·ªáu - V√≠\\nA. ƒêI·ªÄU KHO·∫¢N V√Ä ƒêI·ªÄU KI·ªÜN S·ª¨ D·ª§NG D·ªäCH V·ª§\\nƒêI·ªÄU 4. T√ÄI KHO·∫¢N V√Ä S·ª¨ D·ª§NG D·ªäCH V·ª§\\n4.1. T√†i kho·∫£n V√≠ ƒëi·ªán t·ª≠ GHTKPay\\n4.1.2. N·∫°p/r√∫t, chuy·ªÉn ti·ªÅn, thanh to√°n tr√™n V√≠ ƒëi·ªán t·ª≠', 'relevance_score': 0.9983125}),\n",
       " Document(page_content='T√†i li·ªáu - V√≠\\nA. ƒêI·ªÄU KHO·∫¢N V√Ä ƒêI·ªÄU KI·ªÜN S·ª¨ D·ª§NG D·ªäCH V·ª§\\nƒêI·ªÄU 3. ƒêƒÇNG K√ù\\nƒê·ªÉ m·ªü v√† s·ª≠ d·ª•ng t√†i kho·∫£n V√≠ ƒëi·ªán t·ª≠ GHTKPay v√†/ho·∫∑c ƒëƒÉng k√Ω ch·∫•p nh·∫≠n thanh to√°n th√¥ng qua h·ªá th·ªëng D·ªãch v·ª• thanh to√°n tr·ª±c tuy·∫øn do GHTKPay cung ·ª©ng th√¨ B√™n s·ª≠ d·ª•ng d·ªãch v·ª• ph·∫£i cung c·∫•p v√† ƒë·∫£m b·∫£o t√≠nh x√°c th·ª±c, ƒë·∫ßy ƒë·ªß, ch√≠nh x√°c, v√† c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n ƒë·ªëi v·ªõi c√°c th√¥ng tin, t√†i li·ªáu m√† B√™n s·ª≠ d·ª•ng d·ªãch v·ª• cung c·∫•p t√πy theo y√™u c·∫ßu s·ª≠ d·ª•ng d·ªãch v·ª• m√† B√™n s·ª≠ d·ª•ng d·ªãch v·ª• ƒëƒÉng k√Ω v·ªõi GHTKPay. Quy ƒë·ªãnh KYC c·ª• th·ªÉ ƒë·ªëi v·ªõi t·ª´ng d·ªãch v·ª• v√† ƒë·ªëi t∆∞·ª£ng s·ª≠ d·ª•ng d·ªãch v·ª• nh∆∞ sau:', metadata={'title': 'T√†i li·ªáu - V√≠\\nA. ƒêI·ªÄU KHO·∫¢N V√Ä ƒêI·ªÄU KI·ªÜN S·ª¨ D·ª§NG D·ªäCH V·ª§\\nƒêI·ªÄU 3. ƒêƒÇNG K√ù', 'relevance_score': 0.99768585}),\n",
       " Document(page_content='T√†i li·ªáu - V√≠\\nC. BI·ªÇU PH√ç D·ªäCH V·ª§ GHTKPAY\\n1. N·∫°p ti·ªÅn v√†o v√≠: Mi·ªÖn ph√≠\\n2. R√∫t ti·ªÅn kh·ªèi v√≠: Mi·ªÖn ph√≠\\n3. Chuy·ªÉn ti·ªÅn gi·ªØa c√°c v√≠ GHTKPAY: Mi·ªÖn ph√≠\\n4. Thanh to√°n qua v√≠ (1): Mi·ªÖn ph√≠\\n5. Thanh to√°n qua c·ªïng thanh to√°n (2): 2200 + 2% * gi√° tr·ªã giao d·ªãch. Ph√≠ t·ªëi thi·ªÉu 3.500 VND/ 1 giao d·ªãch\\n(1) (2) √Åp d·ª•ng khi thanh to√°n ƒë∆°n h√†ng, ph√≠ d·ªãch v·ª•, h√≥a ƒë∆°n n·ª£ GHTK, thanh to√°n ti·ªán √≠ch \\n(1) ƒê·ªëi v·ªõi ph√≠ d·ªãch v·ª• c·ªßa m·ªôt s·ªë nh√† cung c·∫•p ti·ªán √≠ch ƒë·∫∑c th√π nh∆∞ ƒë∆°n v·ªã ƒëi·ªán, n∆∞·ªõc, giao th√¥ng, v.v., V√≠ GHTKPAY s·∫Ω th√¥ng b√°o t·ªõi kh√°ch h√†ng tr∆∞·ªõc khi th·ª±c hi·ªán thanh to√°n', metadata={'title': 'T√†i li·ªáu - V√≠\\nC. BI·ªÇU PH√ç D·ªäCH V·ª§ GHTKPAY', 'relevance_score': 0.9975845})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
