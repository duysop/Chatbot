{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trungct/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:107: UserWarning: \n",
      "\n",
      "================================================================================\n",
      "WARNING: Manual override via BNB_CUDA_VERSION env variable detected!\n",
      "BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "Loading CUDA version: BNB_CUDA_VERSION=118\n",
      "================================================================================\n",
      "\n",
      "\n",
      "  warn((f'\\n\\n{\"=\"*80}\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/trungct/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.392 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.0+cu118. CUDA = 8.0. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.22.post7+cu118. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.64s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Viet-Mistral/Vistral-7B-Chat\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`. Luckily, we shall do it for you!\n",
      "Unsloth 2024.5 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head'],\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1u_2rWoM0mrSMaNS9kiUPlF-s98RO66-F\n",
      "To: /home/trungct/Duyborrow/notebook/chatbot.csv\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.08M/7.08M [00:00<00:00, 105MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chatbot.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "\n",
    "url = f\"https://drive.google.com/uc?id=1u_2rWoM0mrSMaNS9kiUPlF-s98RO66-F\"\n",
    "\n",
    "output = \"chatbot.csv\"\n",
    "  # Specify the name of the downloaded file\n",
    "\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gh√©t nh·∫Øm! S√†n TMƒêT GHTK c·∫•m nh√°y m·∫Øt n√†o?</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nV. QU·∫¢N L√ù TH√îNG T...</td>\n",
       "      <td>Th·∫≠t xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ th√¥ng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gh√©t c√°i g√¨ nh·ª©t khi giao d·ªãch tr√™n GHTK?</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nVII. QUY·ªÄN V√Ä NGHƒ®...</td>\n",
       "      <td>Th·∫≠t ti·∫øc khi b·∫°n c·∫£m th·∫•y gh√©t m·ªôt ƒëi·ªÅu g√¨ ƒë√≥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H√†nh vi n√†o b·ªã c·∫•m tr√™n s√†n TMƒêT GHTK nh·ªâ?</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nV. QU·∫¢N L√ù TH√îNG T...</td>\n",
       "      <td>Ch√†o b·∫°n, r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n.\\n\\nTheo quy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Th·∫ø c√°i v·ª• x√¢m nh·∫≠p tr√°i ph√©p v√†o h·ªá th·ªëng GHT...</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nIII. QUY ƒê·ªäNH ƒê·∫¢M ...</td>\n",
       "      <td>Th∆∞a qu√Ω kh√°ch, c·∫£m ∆°n qu√Ω kh√°ch ƒë√£ li√™n h·ªá v·ªõ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T·∫•m chi·∫øu m·ªõi m√† c≈©ng b·ªã c·∫•m nh√∫ng tay nh√∫ng c...</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nIII. QUY ƒê·ªäNH ƒê·∫¢M ...</td>\n",
       "      <td>Th∆∞a anh/ch·ªã,\\nNh·ªØng th√¥ng tin anh/ch·ªã cung c·∫•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>C√≥ bao nhi√™u c√°ch th·ª©c ƒë·ªÉ GHTK th√¥ng b√°o t√¨nh ...</td>\n",
       "      <td>T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...</td>\n",
       "      <td>Xin ch√†o, r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n. Theo th√¥ng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>M√¨nh th√≠ch nh·∫≠n th√¥ng tin giao h√†ng c·ªßa GHTK t...</td>\n",
       "      <td>T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...</td>\n",
       "      <td>Ch√†o b·∫°n, b·∫°n ho√†n to√†n c√≥ th·ªÉ s·ªü h·ªØu cho m√¨nh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>M√¨nh mu·ªën ƒëƒÉng k√Ω d·ªãch v·ª• th√¥ng b√°o t√¨nh tr·∫°ng...</td>\n",
       "      <td>T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...</td>\n",
       "      <td>Th√¢n ch√†o anh/ch·ªã,\\n\\nV·ªÅ d·ªãch v·ª• th√¥ng b√°o t√¨n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>M√¨nh mu·ªën nh·∫≠n th√¥ng b√°o t√¨nh tr·∫°ng ƒë∆°n h√†ng t...</td>\n",
       "      <td>T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...</td>\n",
       "      <td>Ch√†o b·∫°n,\\n\\nC·∫£m ∆°n b·∫°n ƒë√£ l·ª±a ch·ªçn d·ªãch v·ª• Gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>T·ªõ mu·ªën bi·∫øt t√¨nh tr·∫°ng ƒë∆°n h√†ng th√¨ GHTK g·ª≠i ...</td>\n",
       "      <td>T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nI. Quy tr√¨nh giao ...</td>\n",
       "      <td>Ch√†o b·∫°n, n·∫øu b·∫°n mu·ªën bi·∫øt t√¨nh tr·∫°ng ƒë∆°n h√†n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1066 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0            Gh√©t nh·∫Øm! S√†n TMƒêT GHTK c·∫•m nh√°y m·∫Øt n√†o?   \n",
       "1             Gh√©t c√°i g√¨ nh·ª©t khi giao d·ªãch tr√™n GHTK?   \n",
       "2            H√†nh vi n√†o b·ªã c·∫•m tr√™n s√†n TMƒêT GHTK nh·ªâ?   \n",
       "3     Th·∫ø c√°i v·ª• x√¢m nh·∫≠p tr√°i ph√©p v√†o h·ªá th·ªëng GHT...   \n",
       "4     T·∫•m chi·∫øu m·ªõi m√† c≈©ng b·ªã c·∫•m nh√∫ng tay nh√∫ng c...   \n",
       "...                                                 ...   \n",
       "1061  C√≥ bao nhi√™u c√°ch th·ª©c ƒë·ªÉ GHTK th√¥ng b√°o t√¨nh ...   \n",
       "1062  M√¨nh th√≠ch nh·∫≠n th√¥ng tin giao h√†ng c·ªßa GHTK t...   \n",
       "1063  M√¨nh mu·ªën ƒëƒÉng k√Ω d·ªãch v·ª• th√¥ng b√°o t√¨nh tr·∫°ng...   \n",
       "1064  M√¨nh mu·ªën nh·∫≠n th√¥ng b√°o t√¨nh tr·∫°ng ƒë∆°n h√†ng t...   \n",
       "1065  T·ªõ mu·ªën bi·∫øt t√¨nh tr·∫°ng ƒë∆°n h√†ng th√¨ GHTK g·ª≠i ...   \n",
       "\n",
       "                                                context  \\\n",
       "0     T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nV. QU·∫¢N L√ù TH√îNG T...   \n",
       "1     T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nVII. QUY·ªÄN V√Ä NGHƒ®...   \n",
       "2     T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nV. QU·∫¢N L√ù TH√îNG T...   \n",
       "3     T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nIII. QUY ƒê·ªäNH ƒê·∫¢M ...   \n",
       "4     T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nIII. QUY ƒê·ªäNH ƒê·∫¢M ...   \n",
       "...                                                 ...   \n",
       "1061  T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...   \n",
       "1062  T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...   \n",
       "1063  T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...   \n",
       "1064  T√†i li·ªáu - Ch√≠nh s√°ch gi√°\\nIV. D·ªäCH V·ª§ GIA TƒÇN...   \n",
       "1065  T√†i li·ªáu -  Quy ƒë·ªãnh chung\\nI. Quy tr√¨nh giao ...   \n",
       "\n",
       "                                                 answer  \n",
       "0     Th·∫≠t xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ th√¥ng ...  \n",
       "1     Th·∫≠t ti·∫øc khi b·∫°n c·∫£m th·∫•y gh√©t m·ªôt ƒëi·ªÅu g√¨ ƒë√≥...  \n",
       "2     Ch√†o b·∫°n, r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n.\\n\\nTheo quy...  \n",
       "3     Th∆∞a qu√Ω kh√°ch, c·∫£m ∆°n qu√Ω kh√°ch ƒë√£ li√™n h·ªá v·ªõ...  \n",
       "4     Th∆∞a anh/ch·ªã,\\nNh·ªØng th√¥ng tin anh/ch·ªã cung c·∫•...  \n",
       "...                                                 ...  \n",
       "1061  Xin ch√†o, r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n. Theo th√¥ng ...  \n",
       "1062  Ch√†o b·∫°n, b·∫°n ho√†n to√†n c√≥ th·ªÉ s·ªü h·ªØu cho m√¨nh...  \n",
       "1063  Th√¢n ch√†o anh/ch·ªã,\\n\\nV·ªÅ d·ªãch v·ª• th√¥ng b√°o t√¨n...  \n",
       "1064  Ch√†o b·∫°n,\\n\\nC·∫£m ∆°n b·∫°n ƒë√£ l·ª±a ch·ªçn d·ªãch v·ª• Gi...  \n",
       "1065  Ch√†o b·∫°n, n·∫øu b·∫°n mu·ªën bi·∫øt t√¨nh tr·∫°ng ƒë∆°n h√†n...  \n",
       "\n",
       "[1066 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "file_path = 'chatbot.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Now you can work with the DataFrame 'df'\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"B·∫°n l√† tr·ª£ l√Ω AI l·ªãch s·ª≠ ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi chƒÉm s√≥c kh√°ch h√†ng c·ªßa ƒë∆°n v·ªã v·∫≠n chuy·ªÉn Giao H√†ng Ti·∫øt Ki·ªám. \n",
    "B·∫°n ƒë∆∞·ª£c cung c·∫•p c√°c n·ªôi dung ƒë∆∞·ª£c tr√≠ch xu·∫•t sau ƒë√¢y c·ªßa m·ªôt t√†i li·ªáu d√†i v√† m·ªôt c√¢u h·ªèi. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi ƒë√†m tho·∫°i.\n",
    "N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, ch·ªâ c·∫ßn n√≥i \"Xin l·ªói qu√Ω kh√°ch, nh∆∞ng t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p.\n",
    "H√£y tr·∫£ l·ªùi c√¢u h·ªèi nh∆∞ th·ªÉ b·∫°n l√† nh√¢n vi√™n chƒÉm s√≥c kh√°ch h√†ng l·ªãch s·ª± v√† ni·ªÅm n·ªü. V√† ch·ªâ tr·∫£ l·ªùi ng·∫Øn g·ªçn kh√¥ng qu√° 3 c√¢u.\n",
    "\n",
    "### C√¢u h·ªèi:\n",
    "{}\n",
    "\n",
    "### \n",
    "{}\n",
    "\n",
    "### C√¢u tr·∫£ l·ªùi:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    question = examples[\"question\"]\n",
    "    context       = examples[\"context\"]\n",
    "    answer      = examples[\"answer\"]\n",
    "    text = alpaca_prompt.format(question, context, answer) \n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"bleu\")\n",
    "def compute_metrics(eval_preds):\n",
    "    logit, labels = eval_preds\n",
    "    predictions = np.argmax(logit,axis=-1)\n",
    "    true_labels = [\" \".join(str(l) for l in label if l != -100) for label in labels]\n",
    "    true_predictions = [\n",
    "    \" \".join(str(p) for (p, l) in zip(prediction, label) if l != -100)\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return { 'bleu':all_metrics[\"bleu\"],\n",
    "            'precisions':all_metrics[\"precisions\"],\n",
    "            'brevity_penalty':all_metrics[\"brevity_penalty\"],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trungct/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trungct/miniconda3/envs/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "# from unsloth import is_bfloat16_supported\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"<base_dir_location>\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    eval_steps=50,\n",
    "    num_train_epochs = 3.0,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim='adamw_hf',\n",
    "    learning_rate=1e-5,\n",
    "#     fp16=True,\n",
    "    warmup_ratio=0.1,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type='linear',\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset = dataset['test'],\n",
    "    packing=True,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    # peft_config=lora_config,\n",
    "    max_seq_length=2048,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trungct/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 810 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 4 | Total steps = 609\n",
      " \"-____-\"     Number of trainable parameters = 178,130,944\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='609' max='609' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [609/609 1:20:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Precisions</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.405600</td>\n",
       "      <td>1.205938</td>\n",
       "      <td>0.532235</td>\n",
       "      <td>[0.8703857421875, 0.5943575964826575, 0.4445381231671554, 0.34893643031784843]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.041600</td>\n",
       "      <td>0.850925</td>\n",
       "      <td>0.634034</td>\n",
       "      <td>[0.89879150390625, 0.6853321934538349, 0.5581744868035191, 0.4700244498777506]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.769100</td>\n",
       "      <td>0.643692</td>\n",
       "      <td>0.705785</td>\n",
       "      <td>[0.916845703125, 0.7493649242794334, 0.6418743890518084, 0.5626650366748166]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.494249</td>\n",
       "      <td>0.769722</td>\n",
       "      <td>[0.934814453125, 0.8038837322911578, 0.7178030303030303, 0.6507457212713936]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>0.388571</td>\n",
       "      <td>0.818743</td>\n",
       "      <td>[0.9452392578125, 0.8453346360527602, 0.7775659824046921, 0.723239608801956]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.318341</td>\n",
       "      <td>0.851742</td>\n",
       "      <td>[0.95291748046875, 0.8726428920371275, 0.818010752688172, 0.773716381418093]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.294600</td>\n",
       "      <td>0.270003</td>\n",
       "      <td>0.875885</td>\n",
       "      <td>[0.9577392578125, 0.8923668783585735, 0.847983870967742, 0.8121026894865526]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.242500</td>\n",
       "      <td>0.235701</td>\n",
       "      <td>0.893754</td>\n",
       "      <td>[0.96192626953125, 0.9073155837811432, 0.8702468230694037, 0.8400977995110025]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.214493</td>\n",
       "      <td>0.903668</td>\n",
       "      <td>[0.96441650390625, 0.9154127992183684, 0.8824902248289345, 0.8559413202933985]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>0.201622</td>\n",
       "      <td>0.908689</td>\n",
       "      <td>[0.965771484375, 0.919418661455789, 0.888697458455523, 0.8640097799511003]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.195181</td>\n",
       "      <td>0.911627</td>\n",
       "      <td>[0.96636962890625, 0.9218978993649243, 0.8924120234604106, 0.8687163814180929]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.192101</td>\n",
       "      <td>0.912261</td>\n",
       "      <td>[0.96654052734375, 0.9224352711284807, 0.8931207233626588, 0.8697799511002445]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.392 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.0+cu118. CUDA = 8.0. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.22.post7+cu118. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.69s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/trungct/Duyborrow/notebook/unslot.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m conversation \u001b[39m=\u001b[39m alpaca_prompt\u001b[39m.\u001b[39mformat(human, rule, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(conversation, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m out_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minputs[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m768\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     repetition_penalty\u001b[39m=\u001b[39;49m\u001b[39m1.05\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m assistant \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(out_ids[:, inputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m): ], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.70.106.71/home/trungct/Duyborrow/notebook/unslot.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAssistant: \u001b[39m\u001b[39m\"\u001b[39m, assistant) \n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/peft/peft_model.py:1491\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_peft_forward_hooks(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1490\u001b[0m         kwargs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1491\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1492\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1493\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py:1736\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1729\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1730\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1731\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1732\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1733\u001b[0m     )\n\u001b[1;32m   1735\u001b[0m     \u001b[39m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   1737\u001b[0m         input_ids,\n\u001b[1;32m   1738\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1739\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mprepared_logits_warper,\n\u001b[1;32m   1740\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1741\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   1742\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1743\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1744\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1745\u001b[0m     )\n\u001b[1;32m   1747\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1748\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     prepared_logits_warper \u001b[39m=\u001b[39m (\n\u001b[1;32m   1750\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config) \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mdo_sample \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py:2415\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2413\u001b[0m \u001b[39mif\u001b[39;00m do_sample:\n\u001b[1;32m   2414\u001b[0m     probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(next_token_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 2415\u001b[0m     next_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmultinomial(probs, num_samples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m   2416\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2417\u001b[0m     next_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(next_token_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    '/home/trungct/Duyborrow/notebook/<base_dir_location>/checkpoint-609', load_in_4bit=True, device_map='auto',\n",
    ")\n",
    "\n",
    "# conversation = [{\"role\": \"system\", \"content\": prompt }]\n",
    "while True:\n",
    "    human = input(\"Human: \")\n",
    "    rule = input(\"Rules: \")\n",
    "    # if human.lower() == \"reset\":\n",
    "    #     conversation = [{\"role\": \"system\", \"content\": prompt }]\n",
    "    #     print(\"The chat history has been cleared!\")\n",
    "    #     continue\n",
    "\n",
    "    conversation = alpaca_prompt.format(human, rule, \"\")\n",
    "    inputs = tokenizer(conversation, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    out_ids = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=768,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        temperature=0.1,\n",
    "        repetition_penalty=1.05,\n",
    "    )\n",
    "    assistant = tokenizer.batch_decode(out_ids[:, inputs['input_ids'].size(1): ], skip_special_tokens=True)[0].strip()\n",
    "    print(\"Assistant: \", assistant) \n",
    "    # conversation.append({\"role\": \"assistant\", \"content\": assistant })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Em ƒë·ªãnh g·ª≠i m·ªôt ƒë∆°n h√†ng qua GHTK gi√° tr·ªã 2 tri·ªáu ƒë·ªìng nh∆∞ng kh√¥ng c√≥ h√≥a ƒë∆°n ch·ª©ng t·ª´, th√¨ b√™n GHTK s·∫Ω b·ªìi th∆∞·ªùng th·∫ø n√†o cho em?\", # instruction\n",
    "        a, # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
